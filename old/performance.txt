Performance
===========

Getting ideal performance out of any application is a challenging and subtle area of study which is not well-understood by many developers.  The biggest challenge is in dividing the problem space and understanding what actually represents ideal.  When we say performance do we mean raw speed?  Latency?  Concurrency?  Throughput?  But although performance issues can come in many shapes and sizes, almost all of them have the same symptom: your users complain that your site is slow.

Developers building apps on Heroku have the same performance challenges that you would anywhere else.  The good news is that we've created some tools and some safeguards that help identify performance bottlenecks, and then provide clear paths for ways that you can improve the situation if you so choose.

Key concepts
------------

**Dyno** - A [dyno](http://heroku.com/how/dynos) is a single web process running on Heroku.  It is capable of serving a single web request (pageview) at a time.

**Request time** - The length of time spent serving a web request in the dyno.  Request times are generally in the range of 5 - 200ms, with the average being around 50ms.  Rails shows requests times in the logs like this:

    Completed in 27ms (View: 12, DB: 3) | 200 OK [http://localhost/posts]

**Round-trip time** - The time a request takes from the browser or client app.  This number includes the request time, but also includes the time spent traversing the internet and the hops in front of your app include Nginx and Varnish.  You can test round-trip time from your local machine like this:

    :::term
    $ ab -n 1 http://myapp.heroku.com/
    ...
    Time per request:       207.189 [ms] (mean)

**Concurrency** - The number of requests that can be processed simultaneously.  On Heroku, your concurrency level is exactly equal to the number of dynos you have selected on the My Apps -> Resources page for your app.

**Throughput** - Throughput is the number of requests that can be served in a given time period.  You can calculate your app's request-per-second throughput by dividing your concurrency (number of dynos) by your average request time.  For example, if you're running four dynos and have an average request time of 100ms, your maximum throughput will be 4 / 0.100 = 40 requests per second.

**Backlog** - When a web request comes in for your app, the Heroku [routing mesh](http://heroku.com/how/architecture#routing-mesh) looks for an available dyno.  In most cases, a dyno will be ready to take the request immediately.  But if your app is getting more requests than it can handle, all dynos may be busy, in which case the mesh has to hold onto the request and wait for a dyno to come free.  The requests being held in this manner are stored in a queue known as a backlog.

Gregg Pollack has created some excellent visuals to demonstrate backlogs in [this Railslab video](http://content.newrelic.com/railslab/videos/12-ScalingRails-Jesse-Newland.mp4).  Jump to about 60% of the way through when he's describing Nginx + HAProxy.  The Heroku routing mesh behaves similarly to HAProxy with maxconn=1.

A brief backlog is not anything to worry about: a brief spike of traffic may cause requests to sit in the backlog for a few hundred milliseconds, then be forwarded onto a dyno to be processed as usual.  But if the traffic spike is not brief but in fact a longer-term increase in traffic, a growing backlog can indicate that you may soon need to take action.

Further reading: [Backlogs and Request Time](http://adam.blog.heroku.com/past/2009/6/24/backlogs_and_request_time/)

Backlog Too Deep
----------------

If your backlog (as described above) grows too large, the routing mesh will start turning some users away from your site with the message: "Backlog Too Deep."

If you’re using the default of one dyno, your app can only process one request at a time.  This is more than enough for a single user or a few simultaneous users - an app which takes 100ms to process requests can serve 10 pageviews per second.  (This doesn’t count static assets or cached pages, which are served straight from Varnish and never touch your dyno.)

But imagine the case where you’re getting 15 requests per second.  Your single dyno can serve 10 per second.  After one second you will have served 10 requests but have 5 in the backlog waiting to be processed.  This would be fine if no more requests came in, as those 5 requests would be served in the next 0.5 seconds.  However, if you have another 15 requests come in, at the end of the 2nd second you’ll have 5 + 15 – 10 = 10 requests in your backlog.  After 3 seconds, the backlog will be 15 deep, and so on.

If that volume keeps up, the backlog will keep getting deeper.  Eventually, the Heroku routing mesh will detect that you’re increasingly behind, and start turning away some of the requests.

When you are facing this situation, you have three options:

* Increase the number of dynos on your app.  You can calculate how many you need by taking your average time spent serving a request (100ms in the example above) and multiplying it by how many requests you wish to serve per second.  15 requests per second at 100ms per request would require two dynos.  30 requests per second would require 3 dynos, although you might to go with 4 or even 5 to give yourself a little headroom, since traffic often comes in spikes.
* Speed up your request time.  Look in your logs to see where the time is being spent (database?  model code?  views?)  For deeper analysis, a performance monitoring tool like [New Relic](http://newrelic.com/) can be invaluable.  If you’re spending more than 100ms in your dyno processing the request, you probably have room for optimization.  This is a big topic, but some options here include use of memcached, HTTP caching, optimizing your database queries, or using a faster web layer like Rails Metal.
* If you would prefer to not pay for additional dynos or spend time optimizing your app, you can take no action, and be aware that some users will get this error message.

Request Timeout
---------------

The Heroku routing mesh will also detect a different kind of performance problem: long-running requests.  If your dyno takes more than 30 seconds to respond to a request, the mesh will serve a "Request Timeout" page to the user.

Web requests should be typically processed in no more than 500ms, and ideally under 200ms.  So a request which runs 30 seconds is two orders of magnitude slower than a best-practice response!

One possibility may be that you have an infinite loop in your code.  Test locally (perhaps with a copy of the production database pulled down with [Taps](/taps)) and see if you can replicate the problem and fix the bug.

Another possibility is that you are trying to do some sort of long-running task inside your web request, such as:

* Sending an email
* Accessing a remote API (posting to Twitter, querying Flickr, etc)
* Web scraping / crawling
* Rendering an image or PDF
* Heavy computation (generating a fractal, computing a fibonacci sequence, etc)

If so, you should move this heavy lifting into a background job which can run asynchronously from your web request.  See [Background jobs](/background-jobs) for details.

NOTE: Additional concurrency is of no help whatsoever if you are encountering request timeouts.  You can crank your dynos to the maximum and you'll still get a request timeout, since it is a single request that is failing to serve in the correct amount of time.  Extra dynos increase your concurrency, not the speed of your requests.

